{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "marine-glucose",
   "metadata": {},
   "source": [
    "# 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "generic-bones",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_func_name(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        print(func.__name__)        \n",
    "        return func(*args, **kwargs)\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "lightweight-dylan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic_model\n",
      "initialize_beta\n",
      "update_parameters_gradDesc\n",
      "gradient\n",
      "sigmoid\n",
      "J_cost\n",
      "0th iteration, cost is 12.218672214443876\n",
      "gradient\n",
      "sigmoid\n",
      "gradient\n",
      "sigmoid\n",
      "gradient\n",
      "sigmoid\n",
      "gradient\n",
      "sigmoid\n",
      "gradient\n",
      "sigmoid\n",
      "gradient\n",
      "sigmoid\n",
      "gradient\n",
      "sigmoid\n",
      "gradient\n",
      "sigmoid\n",
      "gradient\n",
      "sigmoid\n",
      "gradient\n",
      "sigmoid\n",
      "J_cost\n",
      "10th iteration, cost is 10.617241248040324\n",
      "gradient\n",
      "sigmoid\n",
      "gradient\n",
      "sigmoid\n",
      "gradient\n",
      "sigmoid\n",
      "gradient\n",
      "sigmoid\n",
      "gradient\n",
      "sigmoid\n",
      "gradient\n",
      "sigmoid\n",
      "gradient\n",
      "sigmoid\n",
      "gradient\n",
      "sigmoid\n",
      "gradient\n",
      "sigmoid\n",
      "J_cost\n",
      "8.687180339238743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manlab\\anaconda3\\envs\\aiprice\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:238: RuntimeWarning: Glyph 23494 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\manlab\\anaconda3\\envs\\aiprice\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:238: RuntimeWarning: Glyph 24230 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\manlab\\anaconda3\\envs\\aiprice\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:238: RuntimeWarning: Glyph 21547 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\manlab\\anaconda3\\envs\\aiprice\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:238: RuntimeWarning: Glyph 31958 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\manlab\\anaconda3\\envs\\aiprice\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:238: RuntimeWarning: Glyph 37327 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "C:\\Users\\manlab\\anaconda3\\envs\\aiprice\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:201: RuntimeWarning: Glyph 23494 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\manlab\\anaconda3\\envs\\aiprice\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:201: RuntimeWarning: Glyph 24230 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\manlab\\anaconda3\\envs\\aiprice\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:201: RuntimeWarning: Glyph 21547 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\manlab\\anaconda3\\envs\\aiprice\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:201: RuntimeWarning: Glyph 31958 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "C:\\Users\\manlab\\anaconda3\\envs\\aiprice\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:201: RuntimeWarning: Glyph 37327 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "与原书不同，原书中一个样本xi 为列向量，本代码中一个样本xi为行向量\n",
    "尝试了两种优化方法，梯度下降和牛顿法。两者结果基本相同，不过有时因初始化的原因，\n",
    "会导致牛顿法中海森矩阵为奇异矩阵，np.linalg.inv(hess)会报错。以后有机会再写拟牛顿法吧。\n",
    "\n",
    "원서와 달리 원서의 샘플xi는 열 벡터, 이 코드에서 샘플xi는 행 벡터를 위한 두 가지 최적화 방법을 시도했다.\n",
    "둘의 결과는 거의 같으나, 초기화로 인해 뉴턴법에서 하이젠 행렬이 기이한 행렬로 나타나 np.linalg.inv(hess)가 틀리게 되는 경우도 있다.나중에 기회가 되면 다시 의뉴턴법을 쓰자.\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import linear_model\n",
    "\n",
    "@print_func_name\n",
    "def sigmoid(x):\n",
    "    s = 1 / (1 + np.exp(-x))\n",
    "    return s\n",
    "\n",
    "@print_func_name\n",
    "def J_cost(X, y, beta):\n",
    "    '''\n",
    "    식 3.27의 결과값 리턴\n",
    "    :param X:  sample array, shape(n_samples, n_features)\n",
    "    :param y: array-like, shape (n_samples,)\n",
    "    :param beta: the beta in formula 3.27 , shape(n_features + 1, ) or (n_features + 1, 1)\n",
    "    :return: the result of formula 3.27\n",
    "    '''\n",
    "    X_hat = np.c_[X, np.ones((X.shape[0], 1))]\n",
    "    beta = beta.reshape(-1, 1)\n",
    "    y = y.reshape(-1, 1)\n",
    "\n",
    "    # 식 3.27\n",
    "    Lbeta = -y * np.dot(X_hat, beta) + np.log(1 + np.exp(np.dot(X_hat, beta)))\n",
    "\n",
    "    return Lbeta.sum()\n",
    "\n",
    "@print_func_name\n",
    "def gradient(X, y, beta):\n",
    "    '''\n",
    "    compute the first derivative of J(i.e. formula 3.27) with respect to beta      i.e. formula 3.30\n",
    "    ----------------------------------\n",
    "    :param X: sample array, shape(n_samples, n_features)\n",
    "    :param y: array-like, shape (n_samples,)\n",
    "    :param beta: the beta in formula 3.27 , shape(n_features + 1, ) or (n_features + 1, 1)\n",
    "    :return:\n",
    "    '''\n",
    "    # 기존의 X변수에 1 벡터 추가\n",
    "    X_hat = np.c_[X, np.ones((X.shape[0], 1))]\n",
    "    # 베타와 y를 np.dot이 가능하도록 펴준다\n",
    "    beta = beta.reshape(-1, 1)\n",
    "    y = y.reshape(-1, 1)\n",
    "    # X_hat과 beta를 행렬곱하고 sigmoid 변환\n",
    "    p1 = sigmoid(np.dot(X_hat, beta))\n",
    "\n",
    "    # 기울기는 식 3.30\n",
    "    gra = (-X_hat * (y - p1)).sum(0)\n",
    "\n",
    "    return gra.reshape(-1, 1)\n",
    "\n",
    "@print_func_name\n",
    "def hessian(X, y, beta):\n",
    "    '''\n",
    "    compute the second derivative of J(i.e. formula 3.27) with respect to beta i.e. formula 3.31\n",
    "    ----------------------------------\n",
    "    :param X: sample array, shape(n_samples, n_features)\n",
    "    :param y: array-like, shape (n_samples,)\n",
    "    :param beta: the beta in formula 3.27 , shape(n_features + 1, ) or (n_features + 1, 1)\n",
    "    :return:\n",
    "    '''\n",
    "    X_hat = np.c_[X, np.ones((X.shape[0], 1))]\n",
    "    beta = beta.reshape(-1, 1)\n",
    "    y = y.reshape(-1, 1)\n",
    "\n",
    "    p1 = sigmoid(np.dot(X_hat, beta))\n",
    "\n",
    "    m, n = X.shape\n",
    "    P = np.eye(m) * p1 * (1 - p1)\n",
    "\n",
    "    assert P.shape[0] == P.shape[1]\n",
    "    return np.dot(np.dot(X_hat.T, P), X_hat)\n",
    "\n",
    "@print_func_name\n",
    "def update_parameters_gradDesc(X, y, beta, learning_rate, num_iterations, print_cost):\n",
    "    '''\n",
    "    update parameters with gradient descent method\n",
    "    --------------------------------------------\n",
    "    :param beta:\n",
    "    :param grad:\n",
    "    :param learning_rate:\n",
    "    :return:\n",
    "    '''\n",
    "    for i in range(num_iterations):\n",
    "\n",
    "        grad = gradient(X, y, beta)\n",
    "        beta = beta - learning_rate * grad\n",
    "\n",
    "        if (i % 10 == 0) & print_cost:\n",
    "            print('{}th iteration, cost is {}'.format(i, J_cost(X, y, beta)))\n",
    "\n",
    "    return beta\n",
    "\n",
    "@print_func_name\n",
    "def update_parameters_newton(X, y, beta, num_iterations, print_cost):\n",
    "    '''\n",
    "    update parameters with Newton method\n",
    "    :param beta:\n",
    "    :param grad:\n",
    "    :param hess:\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "\n",
    "        grad = gradient(X, y, beta)\n",
    "        hess = hessian(X, y, beta)\n",
    "        beta = beta - np.dot(np.linalg.inv(hess), grad)\n",
    "\n",
    "        if (i % 10 == 0) & print_cost:\n",
    "            print('{}th iteration, cost is {}'.format(i, J_cost(X, y, beta)))\n",
    "    return beta\n",
    "\n",
    "@print_func_name\n",
    "def initialize_beta(n):\n",
    "    beta = np.random.randn(n + 1, 1) * 0.5 + 1\n",
    "    return beta\n",
    "\n",
    "@print_func_name\n",
    "def logistic_model(X, y, num_iterations=100, learning_rate=1.2, print_cost=False, method='gradDesc'):\n",
    "    '''\n",
    "    :param X:\n",
    "    :param y:~\n",
    "    :param num_iterations:\n",
    "    :param learning_rate:\n",
    "    :param print_cost:\n",
    "    :param method: str 'gradDesc' or 'Newton'\n",
    "    :return:\n",
    "    '''\n",
    "    m, n = X.shape\n",
    "    beta = initialize_beta(n)\n",
    "\n",
    "    if method == 'gradDesc':\n",
    "        return update_parameters_gradDesc(X, y, beta, learning_rate, num_iterations, print_cost)\n",
    "    elif method == 'Newton':\n",
    "        return update_parameters_newton(X, y, beta, num_iterations, print_cost)\n",
    "    else:\n",
    "        raise ValueError('Unknown solver %s' % method)\n",
    "\n",
    "@print_func_name\n",
    "def predict(X, beta):\n",
    "    X_hat = np.c_[X, np.ones((X.shape[0], 1))]\n",
    "    p1 = sigmoid(np.dot(X_hat, beta))\n",
    "\n",
    "    p1[p1 >= 0.5] = 1\n",
    "    p1[p1 < 0.5] = 0\n",
    "\n",
    "    return p1\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data_path = r'./data/watermelon3_0_Ch.csv'\n",
    "    #\n",
    "    data = pd.read_csv(data_path).values\n",
    "\n",
    "    is_good = data[:, 9] == '是'\n",
    "    is_bad = data[:, 9] == '否'\n",
    "\n",
    "    X = data[:, 7:9].astype(float)\n",
    "    y = data[:, 9]\n",
    "\n",
    "    y[y == '是'] = 1\n",
    "    y[y == '否'] = 0\n",
    "    y = y.astype(int)\n",
    "\n",
    "    plt.scatter(data[:, 7][is_good], data[:, 8][is_good], c='k', marker='o')\n",
    "    plt.scatter(data[:, 7][is_bad], data[:, 8][is_bad], c='r', marker='x')\n",
    "\n",
    "    plt.xlabel('密度')\n",
    "    plt.ylabel('含糖量')\n",
    "\n",
    "    # 可视化模型结果\n",
    "    beta = logistic_model(X, y, print_cost=True, method='gradDesc', learning_rate=0.3, num_iterations=20)\n",
    "    w1, w2, intercept = beta\n",
    "    x1 = np.linspace(0, 1)\n",
    "    y1 = -(w1 * x1 + intercept) / w2\n",
    "\n",
    "    ax1, = plt.plot(x1, y1, label=r'my_logistic_gradDesc')\n",
    "\n",
    "    lr = linear_model.LogisticRegression(solver='lbfgs', C=1000)  # 注意sklearn的逻辑回归中，C越大表示正则化程度越低。\n",
    "    lr.fit(X, y)\n",
    "\n",
    "    lr_beta = np.c_[lr.coef_, lr.intercept_]\n",
    "    print(J_cost(X, y, lr_beta))\n",
    "\n",
    "    # 可视化sklearn LogisticRegression 模型结果\n",
    "    w1_sk, w2_sk = lr.coef_[0, :]\n",
    "\n",
    "    x2 = np.linspace(0, 1)\n",
    "    y2 = -(w1_sk * x2 + lr.intercept_) / w2\n",
    "\n",
    "    ax2, = plt.plot(x2, y2, label=r'sklearn_logistic')\n",
    "\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "central-chambers",
   "metadata": {},
   "source": [
    "## 3.3.1 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "strong-assessment",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X : \n",
      " [[0.697 0.46 ]\n",
      " [0.774 0.376]\n",
      " [0.634 0.264]\n",
      " [0.608 0.318]\n",
      " [0.556 0.215]\n",
      " [0.403 0.237]\n",
      " [0.481 0.149]\n",
      " [0.437 0.211]\n",
      " [0.666 0.091]\n",
      " [0.243 0.267]\n",
      " [0.245 0.057]\n",
      " [0.343 0.099]\n",
      " [0.639 0.161]\n",
      " [0.657 0.198]\n",
      " [0.36  0.37 ]\n",
      " [0.593 0.042]\n",
      " [0.719 0.103]]\n",
      "y : \n",
      " [1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0]\n",
      "관측치의 개수 : 17 변수 개수 : 2\n"
     ]
    }
   ],
   "source": [
    "X = data[:, 7:9].astype(float)\n",
    "y = data[:, 9]\n",
    "\n",
    "print(f\"X : \\n {X}\")\n",
    "print(f\"y : \\n {y}\")\n",
    "m, n = X.shape\n",
    "print(f\"관측치의 개수 : {m} 변수 개수 : {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lonely-advance",
   "metadata": {},
   "source": [
    "## 3.3.2 beta 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "civil-expert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "변수 개수에 따른 2개의 가중치와 intercept 추정치 초기화\n",
      "w1 : 1.2269779663054596 w2: 1.5221515014485036 intercept : 2.222467475038255\n"
     ]
    }
   ],
   "source": [
    "@print_func_name\n",
    "def initialize_beta(n):\n",
    "    beta = np.random.randn(n + 1, 1) * 0.5 + 1\n",
    "    return beta\n",
    "\n",
    "# 얼추 계수랑 비슷한 초기 랜덤값 만드려고 한 것 같다 \n",
    "beta = np.random.randn(n + 1, 1) * 0.5 + 1\n",
    "w1, w2, intercept = beta\n",
    "print(f\"변수 개수에 따른 2개의 가중치와 intercept 추정치 초기화\")\n",
    "print(f\"w1 : {w1[0]} w2: {w2[0]} intercept : {intercept[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "velvet-identification",
   "metadata": {},
   "source": [
    "## 3.3.3 gradient로 beta 학습\n",
    "\n",
    "```python\n",
    "@print_func_name\n",
    "def update_parameters_gradDesc(X, y, beta, learning_rate, num_iterations, print_cost):\n",
    "    '''\n",
    "    update parameters with gradient descent method\n",
    "    --------------------------------------------\n",
    "    :param beta:\n",
    "    :param grad:\n",
    "    :param learning_rate:\n",
    "    :return:\n",
    "    '''\n",
    "    for i in range(num_iterations):\n",
    "\n",
    "        grad = gradient(X, y, beta)\n",
    "        beta = beta - learning_rate * grad\n",
    "\n",
    "        if (i % 10 == 0) & print_cost:\n",
    "            print('{}th iteration, cost is {}'.format(i, J_cost(X, y, beta)))\n",
    "\n",
    "    return beta\n",
    "\n",
    "@print_func_name\n",
    "def gradient(X, y, beta):\n",
    "    '''\n",
    "    compute the first derivative of J(i.e. formula 3.27) with respect to beta      i.e. formula 3.30\n",
    "    ----------------------------------\n",
    "    :param X: sample array, shape(n_samples, n_features)\n",
    "    :param y: array-like, shape (n_samples,)\n",
    "    :param beta: the beta in formula 3.27 , shape(n_features + 1, ) or (n_features + 1, 1)\n",
    "    :return:\n",
    "    '''\n",
    "    # 기존의 X변수에 1 벡터 추가\n",
    "    X_hat = np.c_[X, np.ones((X.shape[0], 1))]\n",
    "    # 베타와 y를 np.dot이 가능하도록 펴준다\n",
    "    beta = beta.reshape(-1, 1)\n",
    "    y = y.reshape(-1, 1)\n",
    "    # X_hat과 beta를 행렬곱하고 sigmoid 변환\n",
    "    p1 = sigmoid(np.dot(X_hat, beta))\n",
    "\n",
    "    # 기울기는 식 3.30\n",
    "    gra = (-X_hat * (y - p1)).sum(0)\n",
    "\n",
    "    return gra.reshape(-1, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premium-likelihood",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.3\n",
    "num_iterations=20\n",
    "\n",
    "for i in range(num_iterations):\n",
    "\n",
    "    grad = gradient(X, y, beta)\n",
    "    beta = beta - learning_rate * grad\n",
    "\n",
    "    if (i % 10 == 0) & print_cost:\n",
    "        print('{}th iteration, cost is {}'.format(i, J_cost(X, y, beta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pacific-array",
   "metadata": {},
   "source": [
    "### 3.3.3.1 gradient 함수\n",
    "- 기존의 X변수에 1벡터 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fiscal-lesson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.697, 0.46 , 1.   ],\n",
       "       [0.774, 0.376, 1.   ],\n",
       "       [0.634, 0.264, 1.   ],\n",
       "       [0.608, 0.318, 1.   ],\n",
       "       [0.556, 0.215, 1.   ],\n",
       "       [0.403, 0.237, 1.   ],\n",
       "       [0.481, 0.149, 1.   ],\n",
       "       [0.437, 0.211, 1.   ],\n",
       "       [0.666, 0.091, 1.   ],\n",
       "       [0.243, 0.267, 1.   ],\n",
       "       [0.245, 0.057, 1.   ],\n",
       "       [0.343, 0.099, 1.   ],\n",
       "       [0.639, 0.161, 1.   ],\n",
       "       [0.657, 0.198, 1.   ],\n",
       "       [0.36 , 0.37 , 1.   ],\n",
       "       [0.593, 0.042, 1.   ],\n",
       "       [0.719, 0.103, 1.   ]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 기존의 X변수에 1 벡터 추가\n",
    "X_hat = np.c_[X, np.ones((X.shape[0], 1))]\n",
    "X_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-guarantee",
   "metadata": {},
   "source": [
    "- 베타와 y를 np.dot이 가능하도록 펴준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "measured-contributor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.22697797],\n",
       "        [1.5221515 ],\n",
       "        [2.22246748]]),\n",
       " array([[1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]], dtype=object))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta = beta.reshape(-1, 1)\n",
    "y = y.reshape(-1, 1)\n",
    "beta, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "balanced-batman",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.97763985],\n",
       "       [0.97689832],\n",
       "       [0.96777383],\n",
       "       [0.96930603],\n",
       "       [0.96201833],\n",
       "       [0.95596302],\n",
       "       [0.95432462],\n",
       "       [0.95605308],\n",
       "       [0.96000372],\n",
       "       [0.94916693],\n",
       "       [0.93149217],\n",
       "       [0.94235149],\n",
       "       [0.96272959],\n",
       "       [0.96544304],\n",
       "       [0.96185157],\n",
       "       [0.95320242],\n",
       "       [0.96308225]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_hat과 beta를 행렬곱하고 sigmoid 변환\n",
    "p1 = sigmoid(np.dot(X_hat, beta))\n",
    "p1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subject-package",
   "metadata": {},
   "source": [
    "- gra 변수를 구한 다음 beta = beta - learning_rate * grad 를 통해 beta를 업데이트 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fifty-tobago",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02236015406136027],\n",
       "       [0.023101676508110458],\n",
       "       [0.032226171741622234],\n",
       "       [0.030693967265451594],\n",
       "       [0.03798167118731688],\n",
       "       [0.044036979726897574],\n",
       "       [0.04567538059626597],\n",
       "       [0.04394692333451233],\n",
       "       [-0.9600037152988877],\n",
       "       [-0.9491669307085744],\n",
       "       [-0.9314921683064568],\n",
       "       [-0.9423514889281911],\n",
       "       [-0.9627295874414583],\n",
       "       [-0.9654430356128814],\n",
       "       [-0.9618515720681617],\n",
       "       [-0.953202419119677],\n",
       "       [-0.9630822458874683]], dtype=object)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y-p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "signal-moderator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.122305770983343, 1.256528450614377, 8.30930023895022],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 기울기는 식 3.30\n",
    "gra = (-X_hat * (y - p1)).sum(0)\n",
    "gra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greatest-branch",
   "metadata": {},
   "source": [
    "### 3.3.3.2 update_parameters_gradDesc 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-shell",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increased-release",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defined-latvia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-apartment",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-ladder",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-librarian",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "practical-packaging",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 : \n",
      "[0.         0.02040816 0.04081633 0.06122449 0.08163265 0.10204082\n",
      " 0.12244898 0.14285714 0.16326531 0.18367347 0.20408163 0.2244898\n",
      " 0.24489796 0.26530612 0.28571429 0.30612245 0.32653061 0.34693878\n",
      " 0.36734694 0.3877551  0.40816327 0.42857143 0.44897959 0.46938776\n",
      " 0.48979592 0.51020408 0.53061224 0.55102041 0.57142857 0.59183673\n",
      " 0.6122449  0.63265306 0.65306122 0.67346939 0.69387755 0.71428571\n",
      " 0.73469388 0.75510204 0.7755102  0.79591837 0.81632653 0.83673469\n",
      " 0.85714286 0.87755102 0.89795918 0.91836735 0.93877551 0.95918367\n",
      " 0.97959184 1.        ]\n",
      "\n",
      "y1 = -(w1 * x1 + intercept) / w2 : \n",
      "[-1.46008296 -1.4765336  -1.49298424 -1.50943488 -1.52588552 -1.54233616\n",
      " -1.5587868  -1.57523744 -1.59168808 -1.60813873 -1.62458937 -1.64104001\n",
      " -1.65749065 -1.67394129 -1.69039193 -1.70684257 -1.72329321 -1.73974385\n",
      " -1.75619449 -1.77264513 -1.78909577 -1.80554641 -1.82199705 -1.83844769\n",
      " -1.85489833 -1.87134897 -1.88779961 -1.90425025 -1.92070089 -1.93715153\n",
      " -1.95360217 -1.97005281 -1.98650345 -2.00295409 -2.01940473 -2.03585537\n",
      " -2.05230601 -2.06875665 -2.08520729 -2.10165793 -2.11810857 -2.13455921\n",
      " -2.15100985 -2.16746049 -2.18391113 -2.20036177 -2.21681241 -2.23326305\n",
      " -2.24971369 -2.26616433]\n",
      "\n",
      "w2 : [1.5221515]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x1 = np.linspace(0, 1)\n",
    "y1 = -(w1 * x1 + intercept) / w2\n",
    "print(f\"x1 : \\n{x1}\", end=\"\\n\\n\")\n",
    "print(f\"y1 = -(w1 * x1 + intercept) / w2 : \\n{y1}\", end=\"\\n\\n\")\n",
    "print(f\"w2 : {w2}\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cubic-begin",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-charity",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "##### Update \n",
    "print_cost=True\n",
    "for i in range(num_iterations):\n",
    "\n",
    "    grad = gradient(X, y, beta)\n",
    "    beta = beta - learning_rate * grad\n",
    "\n",
    "    if (i % 10 == 0) & print_cost:\n",
    "        print('{}th iteration, cost is {}'.format(i, J_cost(X, y, beta)))\n",
    "\n",
    "beta\n",
    "# update_parameters_gradDesc(X, y, beta, learning_rate, num_iterations, print_cost)\n",
    "# update_parameters_gradDesc(X, y, beta, learning_rate, num_iterations, print_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-enlargement",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-singing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-trail",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genuine-desperate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "defensive-perfume",
   "metadata": {},
   "source": [
    "```py\n",
    "@print_func_name\n",
    "def gradient(X, y, beta):\n",
    "    '''\n",
    "    compute the first derivative of J(i.e. formula 3.27) with respect to beta      i.e. formula 3.30\n",
    "    ----------------------------------\n",
    "    :param X: sample array, shape(n_samples, n_features)\n",
    "    :param y: array-like, shape (n_samples,)\n",
    "    :param beta: the beta in formula 3.27 , shape(n_features + 1, ) or (n_features + 1, 1)\n",
    "    :return:\n",
    "    '''\n",
    "    X_hat = np.c_[X, np.ones((X.shape[0], 1))]\n",
    "    beta = beta.reshape(-1, 1)\n",
    "    y = y.reshape(-1, 1)\n",
    "    p1 = sigmoid(np.dot(X_hat, beta))\n",
    "\n",
    "    gra = (-X_hat * (y - p1)).sum(0)\n",
    "\n",
    "    return gra.reshape(-1, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "opposite-opposition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.697 0.46 ]\n",
      " [0.774 0.376]\n",
      " [0.634 0.264]\n",
      " [0.608 0.318]\n",
      " [0.556 0.215]\n",
      " [0.403 0.237]\n",
      " [0.481 0.149]\n",
      " [0.437 0.211]\n",
      " [0.666 0.091]\n",
      " [0.243 0.267]\n",
      " [0.245 0.057]\n",
      " [0.343 0.099]\n",
      " [0.639 0.161]\n",
      " [0.657 0.198]\n",
      " [0.36  0.37 ]\n",
      " [0.593 0.042]\n",
      " [0.719 0.103]] [[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]] [[-0.21806062]\n",
      " [ 0.79273253]\n",
      " [ 1.31046743]]\n",
      "X_hat :\n",
      "[[0.697 0.46  1.   ]\n",
      " [0.774 0.376 1.   ]\n",
      " [0.634 0.264 1.   ]\n",
      " [0.608 0.318 1.   ]\n",
      " [0.556 0.215 1.   ]\n",
      " [0.403 0.237 1.   ]\n",
      " [0.481 0.149 1.   ]\n",
      " [0.437 0.211 1.   ]\n",
      " [0.666 0.091 1.   ]\n",
      " [0.243 0.267 1.   ]\n",
      " [0.245 0.057 1.   ]\n",
      " [0.343 0.099 1.   ]\n",
      " [0.639 0.161 1.   ]\n",
      " [0.657 0.198 1.   ]\n",
      " [0.36  0.37  1.   ]\n",
      " [0.593 0.042 1.   ]\n",
      " [0.719 0.103 1.   ]]\n",
      "beta :\n",
      "[[-0.21806062]\n",
      " [ 0.79273253]\n",
      " [ 1.31046743]]\n",
      "y : \n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "sigmoid\n",
      "sigmoid(np.dot(X_hat, beta)) : [[0.82099983]\n",
      " [0.80841685]\n",
      " [0.79923154]\n",
      " [0.8068975 ]\n",
      " [0.79570496]\n",
      " [0.80383953]\n",
      " [0.7897962 ]\n",
      " [0.79938283]\n",
      " [0.77510724]\n",
      " [0.81292637]\n",
      " [0.78620977]\n",
      " [0.7882073 ]\n",
      " [0.78562565]\n",
      " [0.78987343]\n",
      " [0.82131418]\n",
      " [0.77108577]\n",
      " [0.77475066]]\n",
      "gradient ((-X_hat * (y - p1)).sum(0)): [2.6085129093099235 0.6765355496392014 5.529369609349269]\n",
      "return : [[2.6085129093099235]\n",
      " [0.6765355496392014]\n",
      " [5.529369609349269]]\n"
     ]
    }
   ],
   "source": [
    "print(X,y,beta)\n",
    "\n",
    "# gradient\n",
    "\n",
    "X_hat = np.c_[X, np.ones((X.shape[0], 1))]\n",
    "print(f\"X_hat :\\n{X_hat}\")\n",
    "\n",
    "beta = beta.reshape(-1, 1)\n",
    "print(f\"beta :\\n{beta}\")\n",
    "y = y.reshape(-1, 1)\n",
    "print(f\"y : \\n{y}\")\n",
    "p1 = sigmoid(np.dot(X_hat, beta))\n",
    "print(f\"sigmoid(np.dot(X_hat, beta)) : {p1}\")\n",
    "\n",
    "gra = (-X_hat * (y - p1)).sum(0)\n",
    "print(f\"gradient ((-X_hat * (y - p1)).sum(0)): {gra}\")\n",
    "print(f\"return : {gra.reshape(-1,1)}\")\n",
    "# ## return\n",
    "# # gra.reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "concerned-cambridge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.47789186],\n",
       "       [0.44255002],\n",
       "       [0.40114595],\n",
       "       [0.42168354],\n",
       "       [0.38421071],\n",
       "       [0.39360804],\n",
       "       [0.36205248],\n",
       "       [0.38395153],\n",
       "       [0.34124058],\n",
       "       [0.40622705],\n",
       "       [0.33408588],\n",
       "       [0.34670746],\n",
       "       [0.36464948],\n",
       "       [0.37724135],\n",
       "       [0.44469397],\n",
       "       [0.32639693],\n",
       "       [0.34464771]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[[-0.4899270577914314],\n",
    " [-0.5857796015669661],\n",
    " [-0.7061610356408536],\n",
    " [-0.6452602904839044],\n",
    " [-0.7583156144619871],\n",
    " [-0.7291486706055423],\n",
    " [-0.8294840612536099],\n",
    " [-0.759128262508068],\n",
    " [-0.8997000817379205],\n",
    " [-0.6908631080529297],\n",
    " [-0.9246680268547737],\n",
    " [-0.8809166061320955],\n",
    " [-0.8209597040615011],\n",
    " [-0.7803268114740789],\n",
    " [-0.5797958084391752],\n",
    " [-0.9520075855101562],\n",
    " [-0.8879643005726662]]\n",
    "np.log(1+np.exp(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-armor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-weekend",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-coral",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weighted-converter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-kingdom",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-dynamics",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-stockholm",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-bailey",
   "metadata": {},
   "source": [
    "# 3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulated-outreach",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "data_path = r'C:\\Users\\hanmi\\Documents\\xiguabook\\Transfusion.txt'\n",
    "\n",
    "data = np.loadtxt(data_path, delimiter=',').astype(int)\n",
    "\n",
    "X = data[:, :4]\n",
    "y = data[:, 4]\n",
    "\n",
    "m, n = X.shape\n",
    "\n",
    "# normalization\n",
    "X = (X - X.mean(0)) / X.std(0)\n",
    "\n",
    "# shuffle\n",
    "index = np.arange(m)\n",
    "np.random.shuffle(index)\n",
    "\n",
    "X = X[index]\n",
    "y = y[index]\n",
    "\n",
    "# 使用sklarn 中自带的api先\n",
    "# k-10 cross validation\n",
    "lr = linear_model.LogisticRegression(C=2)\n",
    "\n",
    "score = cross_val_score(lr, X, y, cv=10)\n",
    "\n",
    "print(score.mean())\n",
    "\n",
    "# LOO\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "accuracy = 0\n",
    "for train, test in loo.split(X, y):\n",
    "    lr_ = linear_model.LogisticRegression(C=2)\n",
    "    X_train = X[train]\n",
    "    X_test = X[test]\n",
    "    y_train = y[train]\n",
    "    y_test = y[test]\n",
    "    lr_.fit(X_train, y_train)\n",
    "\n",
    "    accuracy += lr_.score(X_test, y_test)\n",
    "\n",
    "print(accuracy / m)\n",
    "\n",
    "# 两者结果几乎一样\n",
    "\n",
    "# 自己写一个试试\n",
    "# k-10\n",
    "# 这里就没考虑最后几个样本了。\n",
    "num_split = int(m / 10)\n",
    "score_my = []\n",
    "for i in range(10):\n",
    "    lr_ = linear_model.LogisticRegression(C=2)\n",
    "    test_index = range(i * num_split, (i + 1) * num_split)\n",
    "    X_test_ = X[test_index]\n",
    "    y_test_ = y[test_index]\n",
    "\n",
    "    X_train_ = np.delete(X, test_index, axis=0)\n",
    "    y_train_ = np.delete(y, test_index, axis=0)\n",
    "\n",
    "    lr_.fit(X_train_, y_train_)\n",
    "\n",
    "    score_my.append(lr_.score(X_test_, y_test_))\n",
    "\n",
    "print(np.mean(score_my))\n",
    "\n",
    "# LOO\n",
    "score_my_loo = []\n",
    "for i in range(m):\n",
    "    lr_ = linear_model.LogisticRegression(C=2)\n",
    "    X_test_ = X[i, :]\n",
    "    y_test_ = y[i]\n",
    "\n",
    "    X_train_ = np.delete(X, i, axis=0)\n",
    "    y_train_ = np.delete(y, i, axis=0)\n",
    "\n",
    "    lr_.fit(X_train_, y_train_)\n",
    "\n",
    "    score_my_loo.append(int(lr_.predict(X_test_.reshape(1, -1)) == y_test_))\n",
    "\n",
    "print(np.mean(score_my_loo))\n",
    "\n",
    "# 结果都是类似\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjusted-narrow",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "virgin-knife",
   "metadata": {},
   "source": [
    "# 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "devoted-requirement",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\hanmi\\\\Documents\\\\xiguabook\\\\watermelon3_0_Ch.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-75a8d410fdca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[0mdata_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr'C:\\Users\\hanmi\\Documents\\xiguabook\\watermelon3_0_Ch.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\aiprice\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    686\u001b[0m     )\n\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 688\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\aiprice\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\aiprice\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 948\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\aiprice\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1180\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1181\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\aiprice\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2010\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\hanmi\\\\Documents\\\\xiguabook\\\\watermelon3_0_Ch.csv'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "class LDA(object):\n",
    "\n",
    "    def fit(self, X_, y_, plot_=False):\n",
    "        pos = y_ == 1\n",
    "        neg = y_ == 0\n",
    "        X0 = X_[neg]\n",
    "        X1 = X_[pos]\n",
    "\n",
    "        u0 = X0.mean(0, keepdims=True)  # (1, n)\n",
    "        u1 = X1.mean(0, keepdims=True)\n",
    "\n",
    "        sw = np.dot((X0 - u0).T, X0 - u0) + np.dot((X1 - u1).T, X1 - u1)\n",
    "        w = np.dot(np.linalg.inv(sw), (u0 - u1).T).reshape(1, -1)  # (1, n)\n",
    "\n",
    "        if plot_:\n",
    "            fig, ax = plt.subplots()\n",
    "            ax.spines['right'].set_color('none')\n",
    "            ax.spines['top'].set_color('none')\n",
    "            ax.spines['left'].set_position(('data', 0))\n",
    "            ax.spines['bottom'].set_position(('data', 0))\n",
    "\n",
    "            plt.scatter(X1[:, 0], X1[:, 1], c='k', marker='o', label='good')\n",
    "            plt.scatter(X0[:, 0], X0[:, 1], c='r', marker='x', label='bad')\n",
    "\n",
    "            plt.xlabel('密度', labelpad=1)\n",
    "            plt.ylabel('含糖量')\n",
    "            plt.legend(loc='upper right')\n",
    "\n",
    "            x_tmp = np.linspace(-0.05, 0.15)\n",
    "            y_tmp = x_tmp * w[0, 1] / w[0, 0]\n",
    "            plt.plot(x_tmp, y_tmp, '#808080', linewidth=1)\n",
    "\n",
    "            wu = w / np.linalg.norm(w)\n",
    "\n",
    "            # 正负样板店\n",
    "            X0_project = np.dot(X0, np.dot(wu.T, wu))\n",
    "            plt.scatter(X0_project[:, 0], X0_project[:, 1], c='r', s=15)\n",
    "            for i in range(X0.shape[0]):\n",
    "                plt.plot([X0[i, 0], X0_project[i, 0]], [X0[i, 1], X0_project[i, 1]], '--r', linewidth=1)\n",
    "\n",
    "            X1_project = np.dot(X1, np.dot(wu.T, wu))\n",
    "            plt.scatter(X1_project[:, 0], X1_project[:, 1], c='k', s=15)\n",
    "            for i in range(X1.shape[0]):\n",
    "                plt.plot([X1[i, 0], X1_project[i, 0]], [X1[i, 1], X1_project[i, 1]], '--k', linewidth=1)\n",
    "\n",
    "            # 中心点的投影\n",
    "            u0_project = np.dot(u0, np.dot(wu.T, wu))\n",
    "            plt.scatter(u0_project[:, 0], u0_project[:, 1], c='#FF4500', s=60)\n",
    "            u1_project = np.dot(u1, np.dot(wu.T, wu))\n",
    "            plt.scatter(u1_project[:, 0], u1_project[:, 1], c='#696969', s=60)\n",
    "\n",
    "            ax.annotate(r'u0 投影点',\n",
    "                        xy=(u0_project[:, 0], u0_project[:, 1]),\n",
    "                        xytext=(u0_project[:, 0] - 0.2, u0_project[:, 1] - 0.1),\n",
    "                        size=13,\n",
    "                        va=\"center\", ha=\"left\",\n",
    "                        arrowprops=dict(arrowstyle=\"->\",\n",
    "                                        color=\"k\",\n",
    "                                        )\n",
    "                        )\n",
    "\n",
    "            ax.annotate(r'u1 投影点',\n",
    "                        xy=(u1_project[:, 0], u1_project[:, 1]),\n",
    "                        xytext=(u1_project[:, 0] - 0.1, u1_project[:, 1] + 0.1),\n",
    "                        size=13,\n",
    "                        va=\"center\", ha=\"left\",\n",
    "                        arrowprops=dict(arrowstyle=\"->\",\n",
    "                                        color=\"k\",\n",
    "                                        )\n",
    "                        )\n",
    "            plt.axis(\"equal\")  # 两坐标轴的单位刻度长度保存一致\n",
    "            plt.show()\n",
    "\n",
    "        self.w = w\n",
    "        self.u0 = u0\n",
    "        self.u1 = u1\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        project = np.dot(X, self.w.T)\n",
    "\n",
    "        wu0 = np.dot(self.w, self.u0.T)\n",
    "        wu1 = np.dot(self.w, self.u1.T)\n",
    "\n",
    "        return (np.abs(project - wu1) < np.abs(project - wu0)).astype(int)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data_path = r'C:\\Users\\hanmi\\Documents\\xiguabook\\watermelon3_0_Ch.csv'\n",
    "\n",
    "    data = pd.read_csv(data_path).values\n",
    "\n",
    "    X = data[:, 7:9].astype(float)\n",
    "    y = data[:, 9]\n",
    "\n",
    "    y[y == '是'] = 1\n",
    "    y[y == '否'] = 0\n",
    "    y = y.astype(int)\n",
    "\n",
    "    lda = LDA()\n",
    "    lda.fit(X, y, plot_=True)\n",
    "    print(lda.predict(X))  # 和逻辑回归的结果一致\n",
    "    print(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-demonstration",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
